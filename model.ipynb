{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the .csv containing the file names and the steering angles\n",
    "\n",
    "### Loading the file names  and steering angles into samples\n",
    "### Splitting samples into train and validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  extractFileName ( abs_path):\n",
    "\n",
    "    import os\n",
    "    if os.name == \"nt\":\n",
    "        split_char = '\\\\' \n",
    "    else:\n",
    "        split_char = '/' \n",
    "    if '\\\\' in abs_path:\n",
    "        # \"  Windows Path \" \n",
    "        image_name = abs_path.split ('\\\\')[-3] \\\n",
    "                            + split_char + abs_path.split ('\\\\')[-2] \\\n",
    "                            + split_char + abs_path.split ('\\\\')[-1]\n",
    "\n",
    "    else:\n",
    "        # \"  Unix Path \" \n",
    "        image_name = abs_path.split ('/')[-3] \\\n",
    "                            + split_char + abs_path.split ('/')[-2] \\\n",
    "                            + split_char + abs_path.split ('/')[-1]\n",
    "    \n",
    "    return image_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the resize shape of the images ( this parameter will be used also in the Generator and in the Model definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_shape = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the PyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  tables import *\n",
    "import tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hdf5_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e76abde744ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhdf5_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'hdf5_file' is not defined"
     ]
    }
   ],
   "source": [
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The file './data/samples.hdf5' is already opened.  Please close it before reopening in write mode.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b137983b5563>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhdf5_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/samples.hdf5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Samples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1-gpu/lib/python3.5/site-packages/tables/file.py\u001b[0m in \u001b[0;36mopen_file\u001b[0;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m                 raise ValueError(\n\u001b[1;32m    316\u001b[0m                     \u001b[0;34m\"The file '%s' is already opened.  Please \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                     \"close it before reopening in write mode.\" % filename)\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;31m# Finally, create the File instance, and return it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The file './data/samples.hdf5' is already opened.  Please close it before reopening in write mode."
     ]
    }
   ],
   "source": [
    "hdf5_file = open_file(\"./data/samples.hdf5\", mode = \"w\", title = \"Samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the two objects as images container:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_training_samples = hdf5_file.create_earray(hdf5_file.root, \\\n",
    "                    'train_img', \\\n",
    "                    tables.UInt8Atom(), \\\n",
    "                    shape=( 0,resized_shape, resized_shape, 3))\n",
    "\n",
    "py_validation_samples      = hdf5_file.create_earray(hdf5_file.root, \\\n",
    "                     'val_img', tables.UInt8Atom(), \\\n",
    "                     shape=( 0,resized_shape, resized_shape, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting .... \n",
      "Reading from logfile = ./data/track2_run1.csv\n",
      "Reading from logfile = ./data/run5.csv\n",
      "Reading from logfile = ./data/run6.csv\n",
      "Reading from logfile = ./data/track2_run4.csv\n",
      "Reading from logfile = ./data/track2_run2.csv\n",
      "Reading from logfile = ./data/run2.csv\n",
      "Reading from logfile = ./data/run1.csv\n",
      "Reading from logfile = ./data/track2_run3.csv\n",
      "Reading from logfile = ./data/run4.csv\n",
      "Reading from logfile = ./data/runx.csv\n",
      "Reading from logfile = ./data/run3.csv\n",
      "Reading from logfile = ./data/run7.csv\n",
      "Reading from logfile = ./data/run9.csv\n",
      "\n",
      "\n",
      "There are 18713 images in total \n",
      "....splitted into training images = 14970  \n",
      "                  val images      = 3743  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "print ( \"Starting .... \")\n",
    "samples_list = []\n",
    "center_image_before = None\n",
    "for name in glob.glob(\"./data/*.csv\"):\n",
    "    print ( \"Reading from logfile = \" + name)\n",
    "    with open(name)  as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for line in reader:\n",
    "                # STEERING ANGLE CALCULATION\n",
    "                samples_list.append([extractFileName( line[0]),\\\n",
    "                                     extractFileName( line[1]),\\\n",
    "                                     extractFileName( line[2]),\\\n",
    "                                     float(line[3])])\n",
    "                \n",
    "samples_list = np.array(samples_list)\n",
    "\n",
    "from random import shuffle\n",
    "shuffle(samples_list)\n",
    "\n",
    "train_list = samples_list[0:int(0.8*len(samples_list))]\n",
    "\n",
    "val_list = samples_list[int(0.8*len(samples_list)):int(1.0*len(samples_list))]\n",
    "\n",
    "\n",
    "print (\"\\n\\nThere are {} images in total \".format(len(samples_list)))\n",
    "print (\"....splitted into training images = {}  \".format(len(train_list)))\n",
    "print (\"                  val images      = {}  \".format(len(val_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_list = train_list[0:100 ]\n",
    "# train_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(line):\n",
    "        preprocessed_samples=[]\n",
    "        # STEERING ANGLE CALCULATION\n",
    "        correction = 0.03 # this is a parameter to tune\n",
    "        center_steering = float(line[3])\n",
    "        left_steering   = center_steering + correction\n",
    "        right_steering  = center_steering - correction\n",
    "\n",
    "        # CENTER IMAGE\n",
    "        center_image = cv2.imread(extractFileName( line[0]))\n",
    "        center_image = cv2.cvtColor (center_image, cv2.COLOR_BGR2RGB)\n",
    "        center_image = cv2.resize(center_image,(resized_shape,resized_shape ))\n",
    "        preprocessed_samples.append([center_image, center_steering ])\n",
    "\n",
    "        #   LEFT IMAGE\n",
    "        left_image = cv2.imread(extractFileName( line[1]))\n",
    "        left_image = cv2.cvtColor (left_image, cv2.COLOR_BGR2RGB)\n",
    "        left_image = cv2.resize(left_image,(resized_shape,resized_shape ))\n",
    "        preprocessed_samples.append([left_image, left_steering ])\n",
    "\n",
    "        #   RIGHT IMAGE\n",
    "        right_image = cv2.imread(extractFileName( line[2]))\n",
    "        right_image = cv2.cvtColor (right_image, cv2.COLOR_BGR2RGB)\n",
    "        right_image = cv2.resize(right_image,(resized_shape,resized_shape ))\n",
    "        preprocessed_samples.append([right_image, right_steering ])\n",
    "\n",
    "\n",
    "        ###\n",
    "        ### IMAGE AUGMENTATION\n",
    "        ###\n",
    "        # augmented center image\n",
    "        preprocessed_samples.append([cv2.flip(center_image,1), center_steering*-1.0 ])\n",
    "\n",
    "        # augmented left image\n",
    "        preprocessed_samples.append([cv2.flip(left_image  ,1), left_steering  *-1.0] )\n",
    "\n",
    "        # augmented right image\n",
    "        preprocessed_samples.append([cv2.flip(right_image,1),  right_steering *-1.0] )\n",
    "        \n",
    "#         print ( \"here 1 {}\".format( np.array(preprocessed_samples).shape))\n",
    "        return np.array(preprocessed_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image preprocessing using the function defined before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Preprocessing the images  .... \n",
      ".. training samples processed 1000\n",
      ".. training samples processed 2000\n",
      ".. training samples processed 3000\n",
      ".. training samples processed 4000\n",
      ".. training samples processed 5000\n",
      ".. training samples processed 6000\n",
      ".. training samples processed 7000\n",
      ".. training samples processed 8000\n",
      ".. training samples processed 9000\n",
      ".. training samples processed 10000\n",
      ".. training samples processed 11000\n",
      ".. training samples processed 12000\n",
      ".. training samples processed 13000\n",
      ".. training samples processed 14000\n",
      ".. validation samples processed 1000\n",
      ".. validation samples processed 2000\n",
      ".. validation samples processed 3000\n",
      "\n",
      "Total training samples 128x128 after augmentation and preprocessing : 89820 \n",
      "\n",
      "Total validation samples 128x128 after augmentation and preprocessing : 22458 \n",
      "... completed\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "print ( \"Starting Preprocessing the images  .... \")\n",
    "train_samples      =  np.array([]).reshape(0,2)\n",
    "validation_samples =  np.array([]).reshape(0,2)\n",
    "training_steering = []\n",
    "val_steering = []\n",
    "\n",
    "for i,sample_line in enumerate(train_list):\n",
    "   for output in data_preprocess(sample_line):\n",
    "#         print (output[0].shape)\n",
    "        py_training_samples.append(output[0][None])\n",
    "        training_steering.append(output[1])\n",
    "          \n",
    "   if i% 1000 == 0 and i> 0 : print(\".. training samples processed {}\".format(i))     \n",
    "   \n",
    "for i,sample_line in enumerate(val_list):\n",
    "   for output in data_preprocess(sample_line):\n",
    "#         print ( \"output[0].shape\" + str(output[0].shape))\n",
    "        py_validation_samples.append(output[0][None])\n",
    "        val_steering.append(output[1])\n",
    "#         print (output[0][None].shape)\n",
    "\n",
    "   if i% 1000 == 0 and i> 0 : print(\".. validation samples processed {}\".format(i))     \n",
    "\n",
    "\n",
    "print (\"\\nTotal training samples {}x{} after augmentation and preprocessing : {} \"\\\n",
    "       .format(resized_shape,resized_shape,\\\n",
    "        str(len(py_training_samples)) ))\n",
    "print (\"\\nTotal validation samples {}x{} after augmentation and preprocessing : {} \"\\\n",
    "       .format(resized_shape,resized_shape,\\\n",
    "        str(len(py_validation_samples)) ))\n",
    "print ( \"... completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the table arrays and copying the labels data inside\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_steering' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ec559dba6c58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpy_training_steerings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhdf5_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf5_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'py_training_steering'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_steering\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpy_val_steerings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhdf5_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdf5_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'py_val_steering'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_steering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_steering' is not defined"
     ]
    }
   ],
   "source": [
    "py_training_steerings = hdf5_file.create_array(hdf5_file.root, 'py_training_steering',training_steering )\n",
    "py_val_steerings = hdf5_file.create_array(hdf5_file.root, 'py_val_steering', val_steering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exploring the dataset ...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEWCAYAAACe8xtsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff34c6e0f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X+cVVW9//HXO8SfqKgQIaCYkl3EtBwVtVuW3UStsK4Z\nZol+TfJHqdWtMPthPyjr3q9XratlWmKZxq1M+uFP0n6KNiqKaCYJCgiCpiL+QMXP/WOtkT2HOTNn\nmH3OmWHez8djHrPP2mvvvfY++5zPXmuvvY4iAjMzszK8qtkFMDOzDYeDipmZlcZBxczMSuOgYmZm\npXFQMTOz0jiomJlZaRxUbIMhaQdJqyQNaHZZukPSQknvaHY5zMrgoLIe/CWwfiRdKulr9Vp/RDwc\nEYMiYk29ttEbSdpJ0suSLuxgXkh6JgfbVZKelHR04fVzedm216vycgvzvFWFv+/kecdKWpPTVkq6\nS9K71reckkbncm5Ukd7ufJE0XNL3JT2St/1gzvP6ivXcWbGeIZJekLSwkNbV/oWkz1SsZ7GkAyV9\nt7DMC5JeLLy+ppB/UGVaTi9u8+WKchwt6SxJPy7kl6RPS3og531Y0jckbVJxrELSPoW0XSQ1/EFE\nBxVbb5VfAs3Um8rSBMcATwAfKH7RFOyRg+2giBgcEZe3vQYOAR4pzB9UWO7dxfSI+Fhh3i0572Dg\nAuBKSYN7WM6qJG0H/AXYHPhXYEvgTcDvgX+ryL65pHGF1x8EFnSw2s7275/AZyRtWblQRJxYOFZf\nB35aWMchhaz/DqwG/k3SawrLF4/1wxXluLyDcp4PTCEdvy1J79lBwIyKfP8E6nbRVisHlR7KVzV/\nlvTf+SrwQUn75/RFkpZLmlzIf5ikO/MV3iJJZ1Ws7xhJD0l6XNIXVKgVSXqVpKmS/pHnz5C0bZ63\nqaQf5/QnJf1V0rAqZV4o6QxJ90p6QtIPJW1amP8uSXPyev4i6Q0Vy35W0t3AMx1cXSofi+V5H+dK\nGidpCnA06YO6StKvcv7tJf1c0gpJCySdWlhXZ/vbdlV6vKSHgd+p4opX0s2Svprfn6clXS9pSC3H\nuoNjVvV9K2x3cr6KfEzSmYX5m0mano/1fZI+I2lxle1U3ecq+UX6svk88CLw7mp56yEiXgZ+BGwB\njKmWr4RyfgJYCXw4Iv4RyZMR8cOI+HZF3h8BkwuvjwEu6+b27gNuAT7ZzeWKJgPfBe4GPrQ+K5A0\nBjgZODoibomIlyJiHilgTZD09kL26cAbJL21B2XuMQeVcuxLOnG2A34CXAnsDexCOpm+I6ntCvAZ\n0kk+GDgMOEnS4QCSxpKu+o4GhgNbAyMK2/k4cDjwVmB70lXf/+R5k3P+UbkcJwLPdVLmo4GDgZ2B\n15E+7Eh6I/AD4KN5Pd8DZqr9leVRueyDI+KlivW+E3hLXufWwJHA4xFxEXA58K18RfZuSa8CfgXc\nlffzIOB0SQfXsL9t3gr8S96XjnwQOA54NbAx8B95P7s61pWqvm8FbwZ2zfvxRUn/ktO/BIwGXku6\nqu7sC6aWfa7c5kjSOTeD9l+mdad0/+o4UqB4qJOsPS3nO4CrchDryo+BSZIG5Pd5EHBrN7cH8AXS\n+Vg1qFcjaUfgQNI5fznp3FkfBwGLI+K2YmJELAJm076W9iyp5jRtPbdVCgeVcizIV0xrgJ+Svti/\nEhGrI+J64AVSgCEibo6IuRHxckTcDVxB+gIBOAL4VUT8KSJeAL4IFNtETwTOjIjFEbEaOAs4Il+Z\nv0gKArtExJqIuD0iVnZS5u9ExKKI+CfpJDwqp08BvhcRt+b1TCdV4ccXlj0/L9tR0HqRVEV/PaCI\nuC8illYpw97A0Ij4SkS8EBEPAt8HJtWwv23OiohnqpQF4IcR8fc8fwawZ07v6li308X71ubLEfFc\nRNxFCpR75PQjga9HxBMRsZjUnFFNLftcNBm4JiKeIF3QTJD06oo8d+Ra55OSOtt2pV8WlntS0gmF\neeMlPQk8D/wX8KGIWN7JumopZ2eGAMvaXkh6Ty7T05Kur8i7GLifFIiOIdVcurt/RMQc4Abgs90o\nZ5sPA3dHxL2kQLpbvmDrriFAtc/P0jy/6HvADpIO6SB/QziolOPRwvRzABFRmTYIQNK+km7KzT1P\nkb5E2k6M7YFFbQtFxLPA44X17Ahc1fYhIFXR1wDDSB+c60ht249I+pakgZ2UeVFh+qG87bZtfKr4\nYSMFye2rLNtORPwO+A7p6nq5pIskbVUl+47A9hXb+lzen672t8uyZMsK08+S3we6PtbtdPG+dWtb\nXZS5ln1uK9NmwPtJV8JExC2kNvoPVmR9U76XMjgiTqV2hxeWGxwR3y/Mmx0Rg4FtgJmk+xwdqqGc\nbbXdyvN1IOkiBdJ7M7xtRkTMzNv/BKkGWuky4FjSxVK1oNLZ/rX5IqlW2mFTcieOYe3+LiHd+1mf\nWuRjFPa7wvA8/xX5QuSr+a8pHFQa7yekD+GoiNia1OaqPG8pqYkAeOXDuF1h2UXAIRUfhE0jYklE\nvBgRX46IscD+wLvovMo9qjC9A/BIYRvTKraxeURcUcjfaY+SiDg/IvYCxpKawT5dZblFpFpecVtb\nRsShXe1vrWXpRFfHulJn71u3tkX7Y1+pln1u815gK+ACScskLSM14TWsCSwiVgEnAR/u5Eq8q3Iu\nJQWP0RXL7cTaJrVZwOG5ybQWPyc1Uz4YEQ/XuMw6IuJvwC+AM7vK20bS/qT7S2cU9ndf4IOd1Dir\n+R0wSoVeXXkbo0itB7M6WOaHpGba93VzW6VwUGm8LYF/RsTz+UQpXlX+DHi30o3+jUlNH8Uvru8C\n03J7LZKGSpqYp98maffcxr2S9CHtrP35FEkjc3vxmaRmO0jNTyfmK3NJ2kLpJvU6vWA6ImnvvOxA\n0n2I5wvleJR0X6HNbcDTSjf+N8tt4OMk7d3V/pagq2NdqbP3rSszSF8w20gaAXysk7zd2efJpPtf\nu5Oa9fYEDgD2kLR7N8rXI7kJ9WLSVX1HOi1nbjb+OWm/t5M0UNJRpIuStu6455BqRT+StHM+N7dk\nbXNmZZmeAd4OfKSEXfwy6b5RV73b2kwmNZuNZe3+jgM2I/XcqllE/J10TlwuaXz+jOxGOl43RsSN\nHSzzEuk+3vo02/WYg0rjnQx8RdLTpA/hK90Cc6+Oj5PaYJcCq4DlpHsaAOeRrpavz8vPJl0BAbyG\n9EW5ktRk8nuqV/shXXlfDzwI/IPcFTEiWoETSE1YTwDzSc0ItdqKFJieIF1lPg78Z553CTA2N+38\nMn+ZvIv0oVtAqspfTLpp3tX+9kgNx7pS1fetBl8htfMvAG4kvU/VtlPTPufgdBBwbkQsK/zdDlxL\nObWVX6n9MxVXdZL3XOBQFXoKdrOcJ5O6xN5Neh8+BhzW1owcEY+RrsyfB/4EPA3MIQX7kzoqUES0\nRsQ/erp/EbGAtT3cOqXUi/JI4NsV+9u2jvV5Xz5G+lz8mHSeXgvcTOoBVs0VVL8XU1cK/0hXr6XU\nY+xJYEw+Kcta70LgIx1d5fRX9TrWVbZ1EjApIpra9dOsHlxT6WUkvVvS5pK2IPWqmQssbG6pNkyN\nOtZKT4IfoPQMyq7Ap4DOrvrN+iwHld5nIumm+SOkm32TwtXJemnUsd6Y1NXzadKN16tJz8iYbXDq\n1vwl6Qek9vLlETEup21LuiE8mnRFeGTut46kM4DjSd0nT42I63L6XsClpJtcvwVOi4hQehjvMmAv\nUrv9ByJiYV12xszMalLPmsqlwISKtKnArIgYQ+oKNxVeebp5ErBbXuYCrR1p9kLSjeMx+a9tnccD\nT0TELsB/A9+s256YmVlN6jYIX0T8QdLoiuSJpKELII1TczOp29tE4Mr84M4CSfOBffIN5a0iYjaA\npMtIQ1hck5c5K6/rZ6ShUNRV88WQIUNi9OjKYpmZWWduv/32xyJiaFf5Gj2y67BYO2THMtY+JTyC\n1HWyzeKc9mKerkxvW2YRpH7Z+Snn7ah4whRAaTDDKQA77LADra2tpeyMmVl/Iamzsd1e0bQb9blG\n0ZAb0BFxUUS0RETL0KFdBlozM1tPjQ4qj0oaDqmbJekhJ4AltB+6YmROW0L74S3a0tstk4c+2JpO\nxm4yM7P6a3RQmcnaJ0onk7pWtqVPkrSJpJ1IN+Rvy01lK/PwBG2/x3B1B+s6Avidu96amTVX3e6p\nSLqCdFN+iNIPEn0JOBuYIel40hAeR0IaMkPSDOBe0oilp8Tan4Q9mbVdiq9h7VhAl5DGAZpPGt6h\nbbh0MzNrkn43TEtLS0v4Rr2ZWfdIuj0iWrrK5yfqzcysNA4qZmZWGgcVMzMrjYOKmZmVptFP1JtZ\nFaOn/qbd64VnH9akkpitP9dUzMysNK6pmDVYsUbSWW2k1nxmvYlrKmZmVhoHFTMzK42DipmZlcZB\nxczMSuOgYmZmpXFQMTOz0jiomJlZaRxUzMysNA4qZmZWGgcVMzMrjYOKmZmVxkHFzMxK46BiZmal\ncVAxM7PSOKiYmVlpHFTMzKw0DipmZlYaBxUzMyuNg4qZmZXGQcXMzErjoGJmZqVxUDEzs9I4qJiZ\nWWkcVMzMrDQOKmZmVhoHFTMzK42DipmZlaYpQUXSJyTNk3SPpCskbSppW0k3SHog/9+mkP8MSfMl\n3S/p4EL6XpLm5nnnS1Iz9sfMzJKGBxVJI4BTgZaIGAcMACYBU4FZETEGmJVfI2lsnr8bMAG4QNKA\nvLoLgROAMflvQgN3xczMKjSr+WsjYDNJGwGbA48AE4Hpef504PA8PRG4MiJWR8QCYD6wj6ThwFYR\nMTsiArissIyZmTVBw4NKRCwB/gt4GFgKPBUR1wPDImJpzrYMGJanRwCLCqtYnNNG5OnKdDMza5Jm\nNH9tQ6p97ARsD2wh6UPFPLnmESVuc4qkVkmtK1asKGu1ZmZWoRnNX+8AFkTEioh4EfgFsD/waG7S\nIv9fnvMvAUYVlh+Z05bk6cr0dUTERRHREhEtQ4cOLXVnzMxsrWYElYeB8ZI2z721DgLuA2YCk3Oe\nycDVeXomMEnSJpJ2It2Qvy03la2UND6v55jCMmZm1gQbNXqDEXGrpJ8BdwAvAXcCFwGDgBmSjgce\nAo7M+edJmgHcm/OfEhFr8upOBi4FNgOuyX9mZtYkDQ8qABHxJeBLFcmrSbWWjvJPA6Z1kN4KjCu9\ngGZmtl78RL2ZmZXGQcXMzErjoGJmZqVxUDEzs9I4qJiZWWkcVMzMrDQOKmZmVhoHFTMzK42DipmZ\nlcZBxczMSuOgYmZmpXFQMTOz0jiomJlZaRxUzMysNA4qZmZWGgcVMzMrjYOKmZmVxkHFzMxK46Bi\nZmalcVAxM7PSOKiYmVlpHFTMzKw0DipmZlYaBxUzMyuNg4qZmZXGQcXMzErjoGJmZqVxUDEzs9I4\nqJiZWWkcVMzMrDQOKmZmVhoHFTMzK42DipmZlcZBxczMSuOgYmZmpWlKUJE0WNLPJP1N0n2S9pO0\nraQbJD2Q/29TyH+GpPmS7pd0cCF9L0lz87zzJakZ+2NmZkmzairnAddGxOuBPYD7gKnArIgYA8zK\nr5E0FpgE7AZMAC6QNCCv50LgBGBM/pvQyJ0wM7P2ugwqkraQ9Ko8/TpJ75E0cH03KGlr4C3AJQAR\n8UJEPAlMBKbnbNOBw/P0RODKiFgdEQuA+cA+koYDW0XE7IgI4LLCMmZm1gS11FT+AGwqaQRwPfBh\n4NIebHMnYAXwQ0l3SrpY0hbAsIhYmvMsA4bl6RHAosLyi3PaiDxdmb4OSVMktUpqXbFiRQ+KbmZm\nnaklqCgingXeB1wQEe8nNUWtr42ANwEXRsQbgWfITV1tcs0jerCNdiLioohoiYiWoUOHlrVaMzOr\nUFNQkbQfcDTwm5w2oJP8XVkMLI6IW/Prn5GCzKO5SYv8f3mevwQYVVh+ZE5bkqcr083MrElqCSqn\nA2cAV0XEPEmvBW5a3w1GxDJgkaRdc9JBwL3ATGByTpsMXJ2nZwKTJG0iaSfSDfnbclPZSknjc6+v\nYwrLmJlZE2zUVYaI+D3we0mb59cPAqf2cLsfBy6XtDHwIHAcKcDNkHQ88BBwZN7ePEkzSIHnJeCU\niFiT13My6f7OZsA1+c/MzJqky6CSm74uAQYBO0jaA/hoRJy8vhuNiDlASwezDqqSfxowrYP0VmDc\n+pbDzMzKVUvz17nAwcDjABFxF6lLsJmZWTs1PfwYEYsqktZ0mNHMzPq1Lpu/SDfV9wciP/R4GukJ\neDMzs3ZqqamcCJxCerBwCbBnfm1mZtZOLb2/HiM9o2JmZtapWnp/nd9B8lNAa0T4uRAzM3tFLc1f\nm5KavB7If28gPb1+vKRz61g2MzPrY2q5Uf8G4IC2Bw4lXQj8EXgzMLeOZTMzsz6mlprKNqQHH9ts\nAWybg8zqupTKzMz6pFpqKt8C5ki6GRDpwcev5+Hqb6xj2czMrI+ppffXJZJ+C+yTkz4XEY/k6U/X\nrWRmZtbn1Ppzws8DS4EngF0keZgWMzNbRy1dij9Ceop+JDAHGA/cAry9vkUzM7O+ppaaymnA3sBD\nEfE24I3Ak3UtlZmZ9Um1BJXnI+J5AEmbRMTfgF27WMbMzPqhWnp/LZY0GPglcIOkJ0g/omVmZtZO\nLb2/3psnz5J0E7A1cG1dS2VmZn1STb2/JG0j6Q3A08Bi/GuLZmbWgVp6f30VOJb0W/Iv5+TAvb/M\nzKxCLfdUjgR2jogX6l0YMzPr22pp/roHGFzvgpiZWd9XS03lG8Cdku6hMIBkRLynbqUyM7M+qZag\nMh34JmmY+5e7yGtmZv1YLUHl2Yjo6NcfzczM2qklqPxR0jeAmbRv/rqjbqUyM7M+qZag8sb8f3wh\nzV2KzcxsHbU8Uf+2RhTEzMz6vqpBRdInO1swIs4pvzhmZtaXdVZT2bJhpTAzsw1C1aASEV9uZEHM\nzKzvq/XnhM3MzLrkoGJmZqVxUDEzs9J0GVQkfb4wvUl9i2NmZn1Z1aAi6bOS9gOOKCTfUtaGJQ2Q\ndKekX+fX20q6QdID+f82hbxnSJov6X5JBxfS95I0N887X5LKKp+ZmXVfZzWVvwHvB14r6Y+Svg9s\nJ2nXkrZ9GnBf4fVUYFZEjAFm5ddIGgtMAnYDJgAXSBqQl7kQOAEYk/8mlFQ2MzNbD50FlSeBzwHz\ngQOB83L6VEl/6clGJY0EDgMuLiRPJI2ITP5/eCH9yohYHRELcnn2kTQc2CoiZkdEAJcVljEzsybo\nLKgcDPwG2Bk4B9gXeCYijouI/Xu43XOBz9B+KP1hEbE0Ty8DhuXpEcCiQr7FOW1Enq5MX4ekKZJa\nJbWuWLGih0U3M7NqqgaViPhcRBwELAR+BAwAhkr6k6Rfre8GJb0LWB4Rt3ey7SANWlmKiLgoIloi\nomXo0KFlrdbMzCrUMkrxdRHRCrRKOiki3ixpSA+2eQDwHkmHApsCW0n6MfCopOERsTQ3bS3P+ZcA\nowrLj8xpS/J0ZbqZmTVJl12KI+IzhZfH5rTH1neDEXFGRIyMiNGkG/C/i4gPkX6vZXLONhm4Ok/P\nBCZJ2kTSTqQb8rflprKVksbnXl/HFJYxM7MmqKWm8oqIuKteBQHOBmZIOh54CDgyb3OepBnAvcBL\nwCkRsSYvczJwKbAZcE3+MzOzJulWUClbRNwM3JynHwcOqpJvGjCtg/RWYFz9SmhmZt3hYVrMzKw0\nDipmZlYaBxUzMyuNg4qZmZXGQcXMzErjoGJmZqVxUDEzs9I4qJiZWWkcVMzMrDQOKmZmVhoHFTMz\nK42DipmZlcZBxczMSuOgYmZmpXFQMTOz0jiomJlZaRxUzMysNA4qZmZWGgcVMzMrjYOKmZmVxkHF\nzMxK46BiZmalcVAxM7PSOKiYmVlpHFTMzKw0DipmZlYaBxUzMyuNg4qZmZVmo2YXwKw/Gz31N80u\nglmpXFMxM7PSOKiYmVlpHFTMzKw0DipmZlYaBxUzMytNw4OKpFGSbpJ0r6R5kk7L6dtKukHSA/n/\nNoVlzpA0X9L9kg4upO8laW6ed74kNXp/zMxsrWbUVF4CPhURY4HxwCmSxgJTgVkRMQaYlV+T500C\ndgMmABdIGpDXdSFwAjAm/01o5I6YmVl7DQ8qEbE0Iu7I008D9wEjgInA9JxtOnB4np4IXBkRqyNi\nATAf2EfScGCriJgdEQFcVljGzMyaoKkPP0oaDbwRuBUYFhFL86xlwLA8PQKYXVhscU57MU9Xppv1\nOj19yLG4/MKzD+tpcczqpmk36iUNAn4OnB4RK4vzcs0jStzWFEmtklpXrFhR1mrNzKxCU4KKpIGk\ngHJ5RPwiJz+am7TI/5fn9CXAqMLiI3Pakjxdmb6OiLgoIloiomXo0KHl7YiZmbXT8Oav3EPrEuC+\niDinMGsmMBk4O/+/upD+E0nnANuTbsjfFhFrJK2UNJ7UfHYM8O0G7YZZl+o1rpebwqw3a8Y9lQOA\nDwNzJc3JaZ8jBZMZko4HHgKOBIiIeZJmAPeSeo6dEhFr8nInA5cCmwHX5D8zM2uShgeViPgTUO15\nkoOqLDMNmNZBeiswrrzSmZlZT/iJejMzK41/T8Wsh3rLPY7eUg7r31xTMTOz0jiomJlZaRxUzMys\nNA4qZmZWGqURUfqPlpaWaG1tbXYxrI+r14ON9eCb9lYGSbdHREtX+VxTMTOz0jiomJlZaRxUzMys\nNA4qZmZWGgcVMzMrjYdpMevEhjb0yYa2P9b7OKiY1agvdSMu6qvltr7JQcWM/vnFW22fXYOxnvA9\nFTMzK42DipmZlcZBxczMSuOgYmZmpfGNeutX3KW2e3y8rLscVKzPKeuLrj/2+KpFWcfFAal/clCx\nXqUeX0QOHuWo93vjwLNhcFCxPqGWwODg0TjdfcbFwaP/8I16MzMrjYOKmZmVxs1f1qe5yat38fth\nrqmYmVlpXFOxhvAVrHWl1nPEN/p7NweV9eTeLB0r87g4EG2Yynxf/TnsWDOPiyKioRtstpaWlmht\nbV2vZWv5MPT2E7snJ1styzoQWLPUoztzXwpa9f5+knR7RLR0lc81FQO6/+Fx8LDepifnZF8KHr2d\ng4qtwwHDNlQ+t+vPQWUDUu1qq9oHyR8ws3XV8nmp9vlyLcdBpa4qT85avuh70uZbS7qZ9Vx3A09P\n1tnXApiDSi/jYGC2Yeivn+U+3/tL0gTgPGAAcHFEnN1Z/nr3/jIz660a0furTz9RL2kA8D/AIcBY\n4ChJY5tbKjOz/qtPBxVgH2B+RDwYES8AVwITm1wmM7N+q6/fUxkBLCq8XgzsW5lJ0hRgSn65StL9\nJZZhCPBYievbUPk4dc3HqDY+TrVZ5zjpmz1a3461ZOrrQaUmEXERcFE91i2ptZZ2xv7Ox6lrPka1\n8XGqTbOOU19v/loCjCq8HpnTzMysCfp6UPkrMEbSTpI2BiYBM5tcJjOzfqtPN39FxEuSPgZcR+pS\n/IOImNfgYtSlWW0D5OPUNR+j2vg41aYpx6nPP6diZma9R19v/jIzs17EQcXMzErjoNJNkt4vaZ6k\nlyVV7a4naYKk+yXNlzS1kWXsDSRtK+kGSQ/k/9tUybdQ0lxJcySt3/g5fUxX54aS8/P8uyW9qRnl\nbLYajtOBkp7K584cSV9sRjmbSdIPJC2XdE+V+Q0/lxxUuu8e4H3AH6pl8PAxAEwFZkXEGGBWfl3N\n2yJiz/7w7EGN58YhwJj8NwW4sKGF7AW68Rn6Yz539oyIrzS0kL3DpcCETuY3/FxyUOmmiLgvIrp6\nIt/Dx6T9nZ6npwOHN7EsvUkt58ZE4LJIZgODJQ1vdEGbzJ+hGkTEH4B/dpKl4eeSg0p9dDR8zIgm\nlaVZhkXE0jy9DBhWJV8AN0q6PQ+ns6Gr5dzw+VP7Mdg/N+tcI2m3xhStT2n4udSnn1OpF0k3Aq/p\nYNaZEXF1o8vTW3V2nIovIiIkVeu7/uaIWCLp1cANkv6Wr77MunIHsENErJJ0KPBLUjOPNZGDSgci\n4h09XEW/GD6ms+Mk6VFJwyNiaa5uL6+yjiX5/3JJV5GaPTbkoFLLudEvzp8udHkMImJlYfq3ki6Q\nNCQiPNjkWg0/l9z8VR8ePibt7+Q8PRlYp4YnaQtJW7ZNA+8kdYTYkNVybswEjsk9d8YDTxWaEvuL\nLo+TpNdIUp7eh/R99njDS9q7Nfxcck2lmyS9F/g2MBT4jaQ5EXGwpO1Jvzx5aC8ZPqbZzgZmSDoe\neAg4EqB4nEj3Wa7K3wsbAT+JiGubVN6GqHZuSDoxz/8u8FvgUGA+8CxwXLPK2yw1HqcjgJMkvQQ8\nB0yKfjZEiKQrgAOBIZIWA18CBkLzziUP02JmZqVx85eZmZXGQcXMzErjoGJmZqVxUDEzs9I4qJiZ\nWWkcVKzfkXRmHmn67jy67b45/XRJm5e4nRMlHVPi+oZIerGtW20P1jO62qi2Zj3lLsXWr0jaDzgH\nODAiVksaAmwcEY9IWgi0lPFEtqSNIuKlnq6nYp0nAR8EXo6It/ZgPaOBX0fEuJKKZvYK11SsvxkO\nPBYRqwEi4rEcUE4FtgduknQTgKR3SrpF0h2S/lfSoJy+l6Tf50Ewr2sb9VXSzZLOVfpdmNMknSXp\nPwrzvinpNkl/l/SvOX1zSTMk3SvpKkm3qvrv9BwFfAoYIWlkW6KkVZKmSbpL0mxJw3L6zvn1XElf\nk7SqcoWSBkj6T0l/zTW3j+b04ZL+kGty97SV16wrDirW31wPjMpf7BdIeitARJwPPEL6bZe35RrM\n54F3RMSbgFbgk5IGkkZUOCIi9gJ+AEwrrH/jiGiJiP/fwbY3ioh9gNNJTz4DnAw8ERFjgS8Ae3VU\naEmjgOERcRswA/hAYfYWwOyI2IM0btoJOf084LyI2J00Om1HjicN3bE3sDdwgqSdSDWi6yJiT2AP\nYE6V5c3acVCxfiUiVpG+uKcAK4CfSjq2g6zjST8O9WdJc0jjl+0I7AqMI42oPIcUeEYWlvtpJ5v/\nRf5/OzA6T7+Z9FshRMQ9wN1Vlv0AKZiQ8x9VmPcC8OsO1r0f8L95+idV1vtO0thQc4Bbge1II/3+\nFThO0lnO0TaxAAABtUlEQVTA7hHxdCf7ZfYKj/1l/U5ErAFuBm6WNJcUMC6tyCbghog4ql2itDsw\nLyL2q7L6ZzrZ9Or8fw3d/+wdBbxG0tH59faSxkTEA8CLhTGvurtuAR+PiOvWmSG9BTgMuFTSORFx\nWTfLbP2QayrWr0jaVVLxNzf2JA14CfA0sGWeng0cIGmXvNwWkl4H3A8MzTf8kTRQPftxqD+zdrDN\nscDuHZT5dcCgiBgREaMjYjTwDdrXVjoyG/j3PD2pSp7rSIMyDmzbVt7XHYFHI+L7wMVA3X/b3DYM\nDirW3wwCpucb43eTmrjOyvMuAq6VdFNErACOBa7I+W4BXp9/2vYI4JuS7iLda9i/B+W5gBSk7gW+\nBswDnqrIcxRwVUXaz+k6qJxOug90N7BLB+uFFDDuBe7I3Yy/R6rpHAjcJelOUtPbebXukPVv7lJs\n1kSSBgADI+J5STsDNwK75uDV03VvDjyXf3lzEnBURPh33q2ufE/FrLk2J3VjHki6v3FyGQEl2wv4\njtIP1jwJ/L+S1mtWlWsqZmZWGt9TMTOz0jiomJlZaRxUzMysNA4qZmZWGgcVMzMrzf8BnDQv7C9X\nJQUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff39b7072b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring the dataset complete.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "print(\"\\nExploring the dataset ...\")\n",
    " \n",
    "# It plots the histogram of an arrray of angles: [0.0,0.1, ..., -0.1]\n",
    "def plot_steering_histogram(steerings, title, num_bins=100):\n",
    "    plt.hist(steerings, num_bins)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Steering Angles')\n",
    "    plt.ylabel('# Images')\n",
    "    plt.show()\n",
    " \n",
    "# # It plots the histogram of an arrray of associative arrays of angles: [{'steering':0.1}, {'steering':0.2}, ..., {'steering':-0.1}]\n",
    "def plot_dataset_histogram(samples, title, num_bins=100):\n",
    "    steerings = []\n",
    "    for item in samples:\n",
    "#         print (item)\n",
    "        steerings.append( float(item) )\n",
    "    plot_steering_histogram(steerings, title, num_bins)\n",
    "\n",
    "samples_before = samples_list[:,3]\n",
    "# Plot the histogram of steering angles before the image augmentation\n",
    "plot_dataset_histogram(samples_before, 'Images per steering angle BEFORE AUGMENTATION', num_bins=100)\n",
    "samples_before = []\n",
    "\n",
    "# Plot the histogram of steering angles after the image augmentation\n",
    "plot_dataset_histogram(training_steering, 'Images per steering angle AFTER AUGMENTATION', num_bins=100)\n",
    "print(\"Exploring the dataset complete.\")\n",
    "samples=[]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition using Keras\n",
    "\n",
    "#### NVIDIA model used\n",
    "#### Image normalization to avoid saturation and make gradients work better.\n",
    "####     Convolution: 5x5, filter: 24, strides: 2x2, activation: ELU\n",
    "####     Convolution: 5x5, filter: 36, strides: 2x2, activation: ELU\n",
    "####     Convolution: 5x5, filter: 48, strides: 2x2, activation: ELU\n",
    "####     Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n",
    "####     Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n",
    "####     Drop out (0.5)\n",
    "####     Fully connected: neurons: 100, activation: ELU\n",
    "####     Fully connected: neurons: 50, activation: ELU\n",
    "####     Fully connected: neurons: 10, activation: ELU\n",
    "####     Fully connected: neurons: 1 (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping (Cropping2D)        (None, 73, 128, 3)        0         \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 73, 128, 3)        0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 35, 62, 24)        1824      \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 16, 29, 36)        21636     \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv2D)               (None, 6, 13, 48)         43248     \n",
      "_________________________________________________________________\n",
      "Conv4 (Conv2D)               (None, 4, 11, 64)         27712     \n",
      "_________________________________________________________________\n",
      "Conv5 (Conv2D)               (None, 2, 9, 64)          36928     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               115300    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 252,219\n",
      "Trainable params: 252,219\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Lambda, Cropping2D, Activation, Dropout, Reshape\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "top_crop = int(resized_shape*10/100)\n",
    "bottom_crop = int(resized_shape*34/100)\n",
    "\n",
    "# Data Preprocessing ( Normalization and mean centering)\n",
    "model.add(Cropping2D(cropping =((bottom_crop,top_crop),(0,0)), input_shape = (resized_shape,resized_shape,3), name =\"cropping\") )\n",
    "model.add(Lambda(lambda x: x/127.5 - 1. , input_shape = (resized_shape,resized_shape,3)))\n",
    "\n",
    "model.add(Conv2D(24, (5, 5), activation='elu', padding='valid',strides=(2, 2), name = \"Conv1\"))\n",
    "\n",
    "model.add(Conv2D(36, (5, 5), activation='elu', padding='valid',strides=(2, 2), name = \"Conv2\"))\n",
    "\n",
    "model.add(Conv2D(48, (5, 5), activation='elu', padding='valid',strides=(2, 2), name = \"Conv3\"))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='elu', padding='valid',strides=(1, 1), name = \"Conv4\"))\n",
    "model.add(Conv2D(64, (3, 3), activation='elu', padding='valid',strides=(1, 1), name = \"Conv5\"))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(100, activation='elu',kernel_initializer='random_uniform',bias_initializer='zeros'))\n",
    "model.add(Dense(50, activation='elu',kernel_initializer='random_uniform',bias_initializer='zeros'))\n",
    "model.add(Dense(10, activation='elu',kernel_initializer='random_uniform',bias_initializer='zeros'))\n",
    "model.add(Dense(1,kernel_initializer='random_uniform',bias_initializer='zeros'))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the generator .\n",
    "### This flushes the files content from disk and return it to Tensorflow for the training fit\n",
    "### The generator is repeated many times ( as many Epochs of training )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Queue Thread process\n",
    "#### Here I am creating a function that will be called in a separate thread . This simply read big Chunks from Disk ( also the Pytable ) , shuffle them, and make them at disposition of a further processer in a Python Queue.\n",
    "#### The size of this two Queue , samples_q and labels_q is defined as batch_size * 100, so for example 3200 samples\n",
    "#### The great thing about Python Queue is that , if we define the maxsize, the put instruction in case the Queue is full, will wait until will be some space free. \n",
    "#### **** This is useful to AVOID TO LOAD THE ENTIRE PYTABLE IN MEMORY ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the batch size:\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "def read_images_into_queue(samples_q, labels_q , samples, labels):\n",
    "    print ( \" reading images into Queue\")\n",
    "    # Define the Queue max size , the Queue.put() automatically do wait until records will be get from \n",
    "    # an other process and will free some space in the queue.\n",
    "#     From docs.python.org:\n",
    "#     The Queue module implements multi-producer, multi-consumer queues. \n",
    "#     It is especially useful in threaded programming when information must be exchanged safely between multiple threads. \n",
    "#     The Queue class in this module implements all the required locking semantics. \n",
    "#     It depends on the availability of thread support in Python; see the threading module.\n",
    "    \n",
    "    numsamples = len(samples)\n",
    "    while 1:  ### remember you need to stop the process !!\n",
    "        print (\"numsamples = \" + str(numsamples))\n",
    "        for offset in range(0, numsamples, batch_size*100):\n",
    "             print ( \"offset = \" + str(offset))\n",
    "             # loading into memory a BIG chunk of data ( 3200 samples )\n",
    "             chunk_batch_samples = samples[offset:offset+batch_size*100]\n",
    "             chunk_batch_labels  = labels[offset:offset+batch_size*100]\n",
    "             print (\"chunk_batch_samples size {}\".format(chunk_batch_samples.shape) )\n",
    "             # shuffle the chunk\n",
    "             chunk_batch_samples,chunk_batch_labels = sklearn.utils.shuffle(chunk_batch_samples,chunk_batch_labels)  \n",
    "             for sample, steering in zip ( chunk_batch_samples,chunk_batch_labels):\n",
    "                samples_q.put(sample)\n",
    "                labels_q.put(steering)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "\n",
    "def generator(samples_q, labels_q, batch_size ):\n",
    "    read_nb = 0\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        \n",
    "        images = []\n",
    "        angles=[]\n",
    "        for i in range(0, batch_size):\n",
    "            images.append(samples_q.get())\n",
    "            angles.append(labels_q.get())\n",
    "\n",
    "        yield np.array(images) , np.array(angles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting the reading process -- from Disk to Memory Queue\n",
    "#### Remember to TERMINATE it !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " reading images into Queue\n",
      "numsamples = 89820\n",
      "offset = 0\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 3200\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 6400\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 9600\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 12800\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 16000\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 19200\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 22400\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 25600\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 28800\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 32000\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 35200\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 38400\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 41600\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 44800\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 48000\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 51200\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 54400\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 57600\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 60800\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 64000\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 67200\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 70400\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 73600\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 76800\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 80000\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 83200\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 86400\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 89600\n",
      "chunk_batch_samples size (220, 128, 128, 3)\n",
      "numsamples = 89820\n",
      "offset = 0\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n",
      "offset = 3200\n",
      "chunk_batch_samples size (3200, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "\n",
    "training_samples_q = Queue(maxsize = batch_size * 100)\n",
    "training_labels_q = Queue(maxsize = batch_size * 100)\n",
    "\n",
    "validation_samples_q = Queue(maxsize = batch_size * 100)\n",
    "validatio_labels_q = Queue(maxsize = batch_size * 100)\n",
    "\n",
    "###############################################################\n",
    "# Training Producer. It loads training data into Queues\n",
    "###############################################################\n",
    "training_producer = Process(target=read_images_into_queue, \n",
    "                            args=(training_samples_q,            # <-- Training images queue\n",
    "                                  training_labels_q,             # <-- Training labels queue\n",
    "                                  py_training_samples,           # <-- Training samples Pytable\n",
    "                                  py_training_steerings))        # <-- Training labels  Pytable\n",
    "training_producer.start()\n",
    "\n",
    "###############################################################\n",
    "# Validation Producer. It loads validation data into Queues\n",
    "###############################################################\n",
    "validation_producer = Process(target=read_images_into_queue, \n",
    "                            args=(validation_samples,              # <-- Training images queue\n",
    "                                  validation_labels,               # <-- Training labels queue\n",
    "                                  py_validation_samples,           # <-- Training samples Pytable\n",
    "                                  py_validation_steerings))        # <-- Training labels  Pytable\n",
    "validation_producer.start()\n",
    "\n",
    "\n",
    "## training_producer.terminate()\n",
    "## validation_producer.terminate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# producer.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Train and Validation generators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note that the traing_generator uses Queue and async thread !\n",
    "train_generator      = generator(training_samples_q, \n",
    "                                 training_labels_q, \n",
    "                                 batch_size)\n",
    "\n",
    "validation_generator = generator(validation_samples_q, \n",
    "                                 validation_steerings_q, \n",
    "                                 batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model using traing_generator and validating with validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2806/2806 [============================>.] - ETA: 0s - loss: 0.0476"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cristianku/anaconda3/envs/carnd-term1-gpu/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/cristianku/anaconda3/envs/carnd-term1-gpu/lib/python3.5/threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cristianku/anaconda3/envs/carnd-term1-gpu/lib/python3.5/site-packages/keras/utils/data_utils.py\", line 568, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "  File \"<ipython-input-47-930f76e5f7cc>\", line 15, in generator\n",
      "    images.append(samples_q.get())\n",
      "AttributeError: 'EArray' object has no attribute 'get'\n",
      "\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-07025d8e5724>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnumber_of_validation_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_val_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mhistory_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m             \u001b[0mnumper_of_train_samples\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumber_of_validation_samples\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1-gpu/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1-gpu/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1119\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1-gpu/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1-gpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2062\u001b[0m                                 \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m                                 \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2064\u001b[0;31m                                 use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   2065\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2066\u001b[0m                             \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1-gpu/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/carnd-term1-gpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2160\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2161\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m                     raise ValueError('Output of generator should be a tuple '\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "numper_of_train_samples      = len(py_training_samples)\n",
    "number_of_validation_samples = len(py_val_samples) \n",
    "\n",
    "history_object = model.fit_generator(train_generator, steps_per_epoch= \\\n",
    "                                     numper_of_train_samples/batch_size, \n",
    "                                     validation_data=validation_generator, \\\n",
    "                                     validation_steps=number_of_validation_samples/batch_size, \n",
    "                                     epochs=epochs, verbose = 1,\\\n",
    "                                     workers=1)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('\\nTotal number of train samples: {} ( shape {}x{})'.format(numper_of_train_samples,resized_shape,resized_shape))\n",
    "print('\\nBatch Size                   : {}'.format(batch_size))\n",
    "print('\\nDuration                     : {}'.format(end_time - start_time))\n",
    "\n",
    "from keras.models import save_model\n",
    "\n",
    "save_model(model, \"selfdrive_model.h5\")\n",
    "print ( \"  \")\n",
    "print ( \" .. model saved to selfdrive_model.h5 \")\n",
    "print ( \"  \")\n",
    "\n",
    "\n",
    "### print the keys contained in the history object\n",
    "print(history_object.history.keys())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# # # Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Convolution broken down in small pieces \n",
    "\n",
    "### Here I am trying to visualize the Convolution Layers to understand visually how many filters I should use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print (\" Loading drive.h5 .......\")\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "\n",
    "modelobj = load_model('drive.h5')\n",
    "print (\" ..... model drive.h5 successfully loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this purpose I am loading a Test image from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load test images\n",
    "import cv2\n",
    "import numpy as np\n",
    "test_images = []\n",
    "\n",
    "image = cv2.imread('./test_images/center1.jpg')\n",
    "image = cv2.cvtColor (image, cv2.COLOR_BGR2RGB)\n",
    "image = cv2.resize(image,(resized_shape,resized_shape ))     \n",
    "test_images.append(image)\n",
    "\n",
    "\n",
    "test_images = np.array(test_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First of all I am looking at the Image Crop if is well done in the right position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call the model to calculate an intermediate layer using the test images\n",
    "layer_name = 'cropping'\n",
    "intermediate_layer_model = Model(inputs=modelobj.input,\n",
    "                                 outputs=modelobj.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(test_images)\n",
    "intermediate_output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the cropped images\n",
    "def show_intermediate_output(image_ori, intermediate_output):\n",
    "    print (intermediate_output.shape)\n",
    "    depth = 0 \n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(20, 100))\n",
    "    new_image = []\n",
    "    plt.subplot(40, 5, 1 )\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image_ori)\n",
    "    for i in range(0,intermediate_output[0,0].shape[0]):\n",
    "           single_output = intermediate_output[:,:,i]\n",
    "#            print ( \"single_output.shape {}\".format(single_output.shape ))\n",
    "#            print ( single_output)\n",
    "           plt.subplot(40, 5, i+2 )\n",
    "           plt.axis('off')\n",
    "           single_output = single_output.astype(np.uint8)\n",
    "           plt.imshow(single_output, cmap='gray')\n",
    "    plt.show()    \n",
    "\n",
    "    \n",
    "show_intermediate_output(test_images[0], intermediate_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the FIRST convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Call the model to calculate an intermediate layer using the test images\n",
    "layer_name = 'Conv1'\n",
    "intermediate_layer_model = Model(input=model.input,\n",
    "                                 output=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(test_images)\n",
    "int_unNorm = (intermediate_output[0]+1) * 127.5\n",
    "show_intermediate_output(test_images[0], int_unNorm ) \n",
    "                            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the SECOND convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Call the model to calculate an intermediate layer using the test images\n",
    "layer_name = 'Conv2'\n",
    "intermediate_layer_model = Model(input=model.input,\n",
    "                                 output=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(test_images)\n",
    "int_unNorm = (intermediate_output[0]+1) * 127.5\n",
    "show_intermediate_output(test_images[0], int_unNorm ) \n",
    "                            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model to calculate an intermediate layer using the test images\n",
    "layer_name = 'Conv3'\n",
    "intermediate_layer_model = Model(input=model.input,\n",
    "                                 output=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(test_images)\n",
    "int_unNorm = (intermediate_output[0]+1) * 127.5\n",
    "show_intermediate_output(test_images[0], int_unNorm ) \n",
    "                            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model to calculate an intermediate layer using the test images\n",
    "layer_name = 'Conv4'\n",
    "intermediate_layer_model = Model(input=model.input,\n",
    "                                 output=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(test_images)\n",
    "int_unNorm = (intermediate_output[0]+1) * 127.5\n",
    "show_intermediate_output(test_images[0], int_unNorm ) \n",
    "                            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from keras.utils.visualize_util import plot\n",
    "from keras.models import load_model\n",
    "%matplotlib inline\n",
    "\n",
    "#visualize the model\n",
    "modelobj = load_model('model.h5')\n",
    "plot (modelobj, to_file='model.png')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(100, 100))\n",
    "image = cv2.imread('model.png')\n",
    "image = cv2.cvtColor (image, cv2.COLOR_BGR2RGB)\n",
    "plt.subplot(5, 5, 1)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
