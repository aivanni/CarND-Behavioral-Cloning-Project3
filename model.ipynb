{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the .csv containing the file names and the steering angles\n",
    "\n",
    "### Loading the file names  and steering angles into samples\n",
    "### Splitting samples into train and validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  extractFileName ( abs_path):\n",
    "\n",
    "    import os\n",
    "    if os.name == \"nt\":\n",
    "        split_char = '\\\\' \n",
    "    else:\n",
    "        split_char = '/' \n",
    "    if '\\\\' in abs_path:\n",
    "        # \"  Windows Path \" \n",
    "        image_name = abs_path.split ('\\\\')[-3] \\\n",
    "                            + split_char + abs_path.split ('\\\\')[-2] \\\n",
    "                            + split_char + abs_path.split ('\\\\')[-1]\n",
    "\n",
    "    else:\n",
    "        # \"  Unix Path \" \n",
    "        image_name = abs_path.split ('/')[-3] \\\n",
    "                            + split_char + abs_path.split ('/')[-2] \\\n",
    "                            + split_char + abs_path.split ('/')[-1]\n",
    "\n",
    "    return image_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the resize shape of the images ( this parameter will be used also in the Generator and in the Model definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_shape = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOADING ALL IMAGES CENTER, LEFT, RIGHT IN A TWO DIMENSIONAL ARRAY:\n",
    "## ( IMAGE, STEERING ANGLE )\n",
    "## EVERYTHING WILL BE KEPT IN MEMORY TO SPEED DATA FLUSHING BETWEEN CPU AND GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the PyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  tables import *\n",
    "import tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file = open_file(\"./data/samples.hdf5\", mode = \"w\", title = \"Samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_order = 'tf'  # 'th' for Theano, 'tf' for Tensorflow\n",
    "\n",
    "\n",
    "data_shape = ( 0,resized_shape, resized_shape, 3)\n",
    "    \n",
    "\n",
    "####h5file.create_array(h5file.root, 'steering_angle', train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the two objects as images container:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_training_samples = hdf5_file.create_earray(hdf5_file.root, \\\n",
    "                    'train_img', \\\n",
    "                    tables.UInt8Atom(), \\\n",
    "                    shape=( 0,resized_shape, resized_shape, 3))\n",
    "\n",
    "py_val_samples      = hdf5_file.create_earray(hdf5_file.root, \\\n",
    "                     'val_img', tables.UInt8Atom(), \\\n",
    "                     shape=( 0,resized_shape, resized_shape, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting .... \n",
      "Reading from logfile = ./data/track2_run1.csv\n",
      "Reading from logfile = ./data/run5.csv\n",
      "Reading from logfile = ./data/run6.csv\n",
      "Reading from logfile = ./data/track2_run4.csv\n",
      "Reading from logfile = ./data/track2_run2.csv\n",
      "Reading from logfile = ./data/run2.csv\n",
      "Reading from logfile = ./data/run1.csv\n",
      "Reading from logfile = ./data/track2_run3.csv\n",
      "Reading from logfile = ./data/run4.csv\n",
      "Reading from logfile = ./data/run3.csv\n",
      "Reading from logfile = ./data/run7.csv\n",
      "Reading from logfile = ./data/run9.csv\n",
      "\n",
      "\n",
      "There are 16934 images in total \n",
      "....splitted into training images = 13547  \n",
      "                  val images      = 3387  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "print ( \"Starting .... \")\n",
    "samples_list = []\n",
    "center_image_before = None\n",
    "for name in glob.glob(\"./data/*.csv\"):\n",
    "    print ( \"Reading from logfile = \" + name)\n",
    "    with open(name)  as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for line in reader:\n",
    "                # STEERING ANGLE CALCULATION\n",
    "                samples_list.append([extractFileName( line[0]),\\\n",
    "                                     extractFileName( line[1]),\\\n",
    "                                     extractFileName( line[2]),\\\n",
    "                                     float(line[3])])\n",
    "                \n",
    "samples_list = np.array(samples_list)\n",
    "\n",
    "from random import shuffle\n",
    "shuffle(samples_list)\n",
    "\n",
    "train_list = samples_list[0:int(0.8*len(samples_list))]\n",
    "\n",
    "val_list = samples_list[int(0.8*len(samples_list)):int(1.0*len(samples_list))]\n",
    "\n",
    "\n",
    "print (\"\\n\\nThere are {} images in total \".format(len(samples_list)))\n",
    "print (\"....splitted into training images = {}  \".format(len(train_list)))\n",
    "print (\"                  val images      = {}  \".format(len(val_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(line):\n",
    "        preprocessed_samples=[]\n",
    "        # STEERING ANGLE CALCULATION\n",
    "        correction = 0.03 # this is a parameter to tune\n",
    "        center_steering = float(line[3])\n",
    "        left_steering   = center_steering + correction\n",
    "        right_steering  = center_steering - correction\n",
    "\n",
    "        # CENTER IMAGE\n",
    "        center_image = cv2.imread(extractFileName( line[0]))\n",
    "        center_image = cv2.cvtColor (center_image, cv2.COLOR_BGR2RGB)\n",
    "        center_image = cv2.resize(center_image,(resized_shape,resized_shape ))\n",
    "        preprocessed_samples.append([center_image, center_steering ])\n",
    "\n",
    "        #   LEFT IMAGE\n",
    "        left_image = cv2.imread(extractFileName( line[1]))\n",
    "        left_image = cv2.cvtColor (left_image, cv2.COLOR_BGR2RGB)\n",
    "        left_image = cv2.resize(left_image,(resized_shape,resized_shape ))\n",
    "        preprocessed_samples.append([left_image, left_steering ])\n",
    "\n",
    "        #   RIGHT IMAGE\n",
    "        right_image = cv2.imread(extractFileName( line[2]))\n",
    "        right_image = cv2.cvtColor (right_image, cv2.COLOR_BGR2RGB)\n",
    "        right_image = cv2.resize(right_image,(resized_shape,resized_shape ))\n",
    "        preprocessed_samples.append([right_image, right_steering ])\n",
    "\n",
    "\n",
    "        ###\n",
    "        ### IMAGE AUGMENTATION\n",
    "        ###\n",
    "        # augmented center image\n",
    "        preprocessed_samples.append([cv2.flip(center_image,1), center_steering*-1.0 ])\n",
    "\n",
    "        # augmented left image\n",
    "        preprocessed_samples.append([cv2.flip(left_image  ,1), left_steering  *-1.0] )\n",
    "\n",
    "        # augmented right image\n",
    "        preprocessed_samples.append([cv2.flip(right_image,1),  right_steering *-1.0] )\n",
    "        \n",
    "#         print ( \"here 1 {}\".format( np.array(preprocessed_samples).shape))\n",
    "        return np.array(preprocessed_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Preprocessing the images  .... \n",
      ".. training samples processed 1000\n",
      ".. training samples processed 2000\n",
      ".. training samples processed 3000\n",
      ".. training samples processed 4000\n",
      ".. training samples processed 5000\n",
      ".. training samples processed 6000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-d20310937322>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m#         print (output[0].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mpy_training_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-165-83a73a4d040a>\u001b[0m in \u001b[0;36mdata_preprocess\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#   RIGHT IMAGE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mright_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextractFileName\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mright_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mright_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mright_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresized_shape\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# http://machinelearninguru.com/deep_learning/data_preparation/hdf5/hdf5.html\n",
    "    \n",
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "print ( \"Starting Preprocessing the images  .... \")\n",
    "train_samples      =  np.array([]).reshape(0,2)\n",
    "validation_samples =  np.array([]).reshape(0,2)\n",
    "training_steering = []\n",
    "val_steering = []\n",
    "\n",
    "for i,sample_line in enumerate(train_list):\n",
    "   for output in data_preprocess(sample_line):\n",
    "#         print (output[0].shape)\n",
    "        py_training_samples.append(output[0][None])\n",
    "        training_steering.append(output[1])\n",
    "          \n",
    "   if i% 1000 == 0 and i> 0 : print(\".. training samples processed {}\".format(i))     \n",
    "   \n",
    "for i,sample_line in enumerate(val_list):\n",
    "   for output in data_preprocess(sample_line):\n",
    "#         print ( \"output[0].shape\" + str(output[0].shape))\n",
    "        py_val_samples.append(output[0][None])\n",
    "        val_steering.append(output[1])\n",
    "#         print (output[0][None].shape)\n",
    "\n",
    "   if i% 1000 == 0 and i> 0 : print(\".. validation samples processed {}\".format(i))     \n",
    "\n",
    "\n",
    "print (\"\\nTotal training samples {}x{} after augmentation and preprocessing : {} \"\\\n",
    "       .format(resized_shape,resized_shape,\\\n",
    "        str(len(py_training_samples)) ))\n",
    "print (\"\\nTotal validation samples {}x{} after augmentation and preprocessing : {} \"\\\n",
    "       .format(resized_shape,resized_shape,\\\n",
    "        str(len(py_val_samples)) ))\n",
    "print ( \"... completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the table arrays and copying the labels data inside\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_training_steerings = hdf5_file.create_array(hdf5_file.root, 'py_training_steering',training_steering )\n",
    "py_val_steerings = hdf5_file.create_array(hdf5_file.root, 'py_val_steering', val_steering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import csv\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import glob\n",
    "\n",
    "# print ( \"Starting .... \")\n",
    "# samples = []\n",
    "# center_image_before = None\n",
    "# for name in glob.glob(\"./data/run1.csv\"):\n",
    "#     print ( \"Reading from logfile = \" + name)\n",
    "#     with open(name)  as csvfile:\n",
    "#             reader = csv.reader(csvfile)\n",
    "#             for line in reader:\n",
    "#                 # STEERING ANGLE CALCULATION\n",
    "#                 correction = 0.03 # this is a parameter to tune\n",
    "#                 center_steering = float(line[3])\n",
    "#                 left_steering   = center_steering + correction\n",
    "#                 right_steering  = center_steering - correction\n",
    "                \n",
    "#                 # CENTER IMAGE\n",
    "#                 center_image = cv2.imread(extractFileName( line[0]))\n",
    "#                 center_image = cv2.cvtColor (center_image, cv2.COLOR_BGR2RGB)\n",
    "#                 center_image = cv2.resize(center_image,(resized_shape,resized_shape ))\n",
    "#                 samples.append([center_image, center_steering ])\n",
    "                \n",
    "#                 #   LEFT IMAGE\n",
    "#                 left_image = cv2.imread(extractFileName( line[1]))\n",
    "#                 left_image = cv2.cvtColor (left_image, cv2.COLOR_BGR2RGB)\n",
    "#                 left_image = cv2.resize(left_image,(resized_shape,resized_shape ))\n",
    "#                 samples.append([left_image, left_steering ])\n",
    "                \n",
    "#                 #   RIGHT IMAGE\n",
    "#                 right_image = cv2.imread(extractFileName( line[2]))\n",
    "#                 right_image = cv2.cvtColor (right_image, cv2.COLOR_BGR2RGB)\n",
    "#                 right_image = cv2.resize(right_image,(resized_shape,resized_shape ))\n",
    "#                 samples.append([right_image, right_steering ])\n",
    "\n",
    "                \n",
    "#                 ###\n",
    "#                 ### IMAGE AUGMENTATION\n",
    "#                 ###\n",
    "#                 # augmented center image\n",
    "#                 samples.append([cv2.flip(center_image,1), center_steering*-1.0 ])\n",
    "\n",
    "#                 # augmented left image\n",
    "#                 samples.append([cv2.flip(left_image  ,1), left_steering  *-1.0] )\n",
    "\n",
    "#                 # augmented right image\n",
    "#                 samples.append([cv2.flip(right_image,1),  right_steering *-1.0] )\n",
    "\n",
    "                \n",
    "#                 ###\n",
    "#                 ### IMAGE AUGMENTATION 2 - \n",
    "#                 ### Adding images of frames before... with the actual steering wheel angle\n",
    "#                 if center_image_before is not None:\n",
    "#                     samples.append([center_image_before, center_steering ])\n",
    " \n",
    "#                 center_image_before = center_image\n",
    "    \n",
    "\n",
    "# samples =np.array(samples)\n",
    "                \n",
    "# print (\"\\nTotal samples {}x{} loaded in memory  : {} , size occupied in memory : {} Mb\"\\\n",
    "#        .format(resized_shape,resized_shape,\\\n",
    "#         str(len(samples)*3), int(samples.nbytes/8/1024)) )\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "# print (\"\\nTrain samples = {}\".format(train_samples.shape[0]*3))\n",
    "# print (\"\\nValidation samples  = {}\".format(validation_samples.shape[0]*3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "print(\"\\nExploring the dataset ...\")\n",
    " \n",
    "# It plots the histogram of an arrray of angles: [0.0,0.1, ..., -0.1]\n",
    "def plot_steering_histogram(steerings, title, num_bins=100):\n",
    "    plt.hist(steerings, num_bins)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Steering Angles')\n",
    "    plt.ylabel('# Images')\n",
    "    plt.show()\n",
    " \n",
    "# # It plots the histogram of an arrray of associative arrays of angles: [{'steering':0.1}, {'steering':0.2}, ..., {'steering':-0.1}]\n",
    "def plot_dataset_histogram(samples, title, num_bins=100):\n",
    "    steerings = []\n",
    "    for item in samples:\n",
    "#         print (item)\n",
    "        steerings.append( float(item) )\n",
    "    plot_steering_histogram(steerings, title, num_bins)\n",
    "\n",
    "samples_before = samples_list[:,3]\n",
    "# Plot the histogram of steering angles before the image augmentation\n",
    "plot_dataset_histogram(samples_before, 'Images per steering angle BEFORE AUGMENTATION', num_bins=100)\n",
    "samples_before = []\n",
    "\n",
    "# Plot the histogram of steering angles after the image augmentation\n",
    "plot_dataset_histogram(training_steering, 'Images per steering angle AFTER AUGMENTATION', num_bins=100)\n",
    "print(\"Exploring the dataset complete.\")\n",
    "samples=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the generator .\n",
    "### This flushes the files content from disk and return it to Tensorflow for the training fit\n",
    "### The generator is repeated many times ( as many Epochs of training )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "\n",
    "def generator(samples, labels, batch_size, shape ):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "#         sklearn.utils.shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size*100):\n",
    "            # loading into memory a BIG chunk of data ( 3200 samples )\n",
    "            chunk_batch_samples = samples[offset:offset+batch_size*100]\n",
    "            chunk_batch_labels  = labels[offset:offset+batch_size*100]\n",
    "            # shuffling them \n",
    "            chunk_batch_samples,chunk_batch_labels = sklearn.utils.shuffle(chunk_batch_samples,chunk_batch_labels)\n",
    "            \n",
    "#             print (\" offset = {}, len(chunk_batch_samples) {}\".format(offset, len(chunk_batch_samples)))\n",
    "            images = []\n",
    "            angles = []\n",
    "            # looping on the Chunk of 3200 samples , extracting 32 elements ( batch_size) every time\n",
    "            for offset2 in range(0, len(chunk_batch_samples), batch_size):\n",
    "                chunk_batch_samples = chunk_batch_samples[offset2:offset2+batch_size]\n",
    "                chunk_batch_labels  = chunk_batch_labels[offset2:offset2+batch_size]\n",
    "\n",
    "                for batch_sample, batch_label in zip(chunk_batch_samples, chunk_batch_labels):\n",
    "\n",
    "                    images.append(batch_sample)\n",
    "                    angles.append(batch_label)\n",
    "\n",
    "                yield sklearn.utils.shuffle(np.array(images) , np.array(angles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (generator(py_training_samples, py_training_steerings, batch_size, shape=resized_shape)[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NVIDIA model used\n",
    "#### Image normalization to avoid saturation and make gradients work better.\n",
    "####     Convolution: 5x5, filter: 24, strides: 2x2, activation: ELU\n",
    "####     Convolution: 5x5, filter: 36, strides: 2x2, activation: ELU\n",
    "####     Convolution: 5x5, filter: 48, strides: 2x2, activation: ELU\n",
    "####     Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n",
    "####     Convolution: 3x3, filter: 64, strides: 1x1, activation: ELU\n",
    "####     Drop out (0.5)\n",
    "####     Fully connected: neurons: 100, activation: ELU\n",
    "####     Fully connected: neurons: 50, activation: ELU\n",
    "####     Fully connected: neurons: 10, activation: ELU\n",
    "####     Fully connected: neurons: 1 (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cropping (Cropping2D)        (None, 73, 128, 3)        0         \n",
      "_________________________________________________________________\n",
      "lambda_10 (Lambda)           (None, 73, 128, 3)        0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 35, 62, 24)        1824      \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 16, 29, 36)        21636     \n",
      "_________________________________________________________________\n",
      "Conv3 (Conv2D)               (None, 6, 13, 48)         43248     \n",
      "_________________________________________________________________\n",
      "Conv4 (Conv2D)               (None, 4, 11, 64)         27712     \n",
      "_________________________________________________________________\n",
      "Conv5 (Conv2D)               (None, 2, 9, 64)          36928     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 100)               115300    \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 252,219\n",
      "Trainable params: 252,219\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Lambda, Cropping2D, Activation, Dropout, Reshape\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "top_crop = int(resized_shape*10/100)\n",
    "bottom_crop = int(resized_shape*34/100)\n",
    "\n",
    "# Data Preprocessing ( Normalization and mean centering)\n",
    "model.add(Cropping2D(cropping =((bottom_crop,top_crop),(0,0)), input_shape = (resized_shape,resized_shape,3), name =\"cropping\") )\n",
    "model.add(Lambda(lambda x: x/127.5 - 1. , input_shape = (resized_shape,resized_shape,3)))\n",
    "\n",
    "model.add(Conv2D(24, (5, 5), activation='elu', padding='valid',strides=(2, 2), name = \"Conv1\"))\n",
    "\n",
    "model.add(Conv2D(36, (5, 5), activation='elu', padding='valid',strides=(2, 2), name = \"Conv2\"))\n",
    "\n",
    "model.add(Conv2D(48, (5, 5), activation='elu', padding='valid',strides=(2, 2), name = \"Conv3\"))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='elu', padding='valid',strides=(1, 1), name = \"Conv4\"))\n",
    "model.add(Conv2D(64, (3, 3), activation='elu', padding='valid',strides=(1, 1), name = \"Conv5\"))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(100, activation='elu',kernel_initializer='random_uniform',bias_initializer='zeros'))\n",
    "model.add(Dense(50, activation='elu',kernel_initializer='random_uniform',bias_initializer='zeros'))\n",
    "model.add(Dense(10, activation='elu',kernel_initializer='random_uniform',bias_initializer='zeros'))\n",
    "model.add(Dense(1,kernel_initializer='random_uniform',bias_initializer='zeros'))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the training and printing a statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the batch size and epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Train and Validation generators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator      = generator(py_training_samples, py_training_steerings, batch_size, shape=resized_shape)\n",
    "validation_generator = generator(py_val_samples, py_val_steerings, batch_size, shape=resized_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model using traing_generator and validating with validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2541/2540 [==============================] - 44s - loss: 0.0117 - val_loss: 0.1117\n",
      "\n",
      "Total number of train samples: 81282 ( shape 128x128)\n",
      "\n",
      "Batch Size                   : 32\n",
      "\n",
      "Duration                     : 0:00:44.880445\n",
      "  \n",
      " .. model saved to drive.h5 \n",
      "  \n",
      "dict_keys(['val_loss', 'loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7eff61c3ffd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "numper_of_train_samples      = len(py_training_samples)\n",
    "number_of_validation_samples = len(py_val_samples) \n",
    "\n",
    "history_object = model.fit_generator(train_generator, steps_per_epoch= \\\n",
    "            numper_of_train_samples/batch_size, validation_data=validation_generator, \\\n",
    "            validation_steps=number_of_validation_samples/batch_size, epochs=epochs, verbose = 1)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('\\nTotal number of train samples: {} ( shape {}x{})'.format(numper_of_train_samples,resized_shape,resized_shape))\n",
    "print('\\nBatch Size                   : {}'.format(batch_size))\n",
    "print('\\nDuration                     : {}'.format(end_time - start_time))\n",
    "\n",
    "from keras.models import save_model\n",
    "\n",
    "save_model(model, \"drive.h5\")\n",
    "print ( \"  \")\n",
    "print ( \" .. model saved to drive.h5 \")\n",
    "print ( \"  \")\n",
    "\n",
    "\n",
    "### print the keys contained in the history object\n",
    "print(history_object.history.keys())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# # # Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Convolution broken down in small pieces \n",
    "\n",
    "### Here I am trying to visualize the Convolution Layers to understand visually how many filters I should use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading drive.h5 .......\n",
      " ..... model drive.h5 successfully loaded\n"
     ]
    }
   ],
   "source": [
    "print (\" Loading drive.h5 .......\")\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "\n",
    "modelobj = load_model('drive.h5')\n",
    "print (\" ..... model drive.h5 successfully loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this purpose I am loading a Test image from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load test images\n",
    "import cv2\n",
    "import numpy as np\n",
    "test_images = []\n",
    "\n",
    "image = cv2.imread('./test_images/center1.jpg')\n",
    "image = cv2.cvtColor (image, cv2.COLOR_BGR2RGB)\n",
    "image = cv2.resize(image,(resized_shape,resized_shape ))     \n",
    "test_images.append(image)\n",
    "\n",
    "\n",
    "test_images = np.array(test_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First of all I am looking at the Image Crop if is well done in the right position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call the model to calculate an intermediate layer using the test images\n",
    "layer_name = 'cropping'\n",
    "intermediate_layer_model = Model(inputs=modelobj.input,\n",
    "                                 outputs=modelobj.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(test_images)\n",
    "intermediate_output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the cropped images\n",
    "def show_intermediate_output(image_ori, intermediate_output):\n",
    "    print (intermediate_output.shape)\n",
    "    depth = 0 \n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(20, 100))\n",
    "    new_image = []\n",
    "    plt.subplot(40, 5, 1 )\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image_ori)\n",
    "    for i in range(0,intermediate_output[0,0].shape[0]):\n",
    "           single_output = intermediate_output[:,:,i]\n",
    "#            print ( \"single_output.shape {}\".format(single_output.shape ))\n",
    "#            print ( single_output)\n",
    "           plt.subplot(40, 5, i+2 )\n",
    "           plt.axis('off')\n",
    "           single_output = single_output.astype(np.uint8)\n",
    "           plt.imshow(single_output, cmap='gray')\n",
    "    plt.show()    \n",
    "\n",
    "    \n",
    "show_intermediate_output(test_images[0], intermediate_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the FIRST convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Call the model to calculate an intermediate layer using the test images\n",
    "layer_name = 'Conv1'\n",
    "intermediate_layer_model = Model(input=model.input,\n",
    "                                 output=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(test_images)\n",
    "int_unNorm = (intermediate_output[0]+1) * 127.5\n",
    "show_intermediate_output(test_images[0], int_unNorm ) \n",
    "                            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the SECOND convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Call the model to calculate an intermediate layer using the test images\n",
    "layer_name = 'Conv2'\n",
    "intermediate_layer_model = Model(input=model.input,\n",
    "                                 output=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(test_images)\n",
    "int_unNorm = (intermediate_output[0]+1) * 127.5\n",
    "show_intermediate_output(test_images[0], int_unNorm ) \n",
    "                            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model to calculate an intermediate layer using the test images\n",
    "layer_name = 'Conv3'\n",
    "intermediate_layer_model = Model(input=model.input,\n",
    "                                 output=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(test_images)\n",
    "int_unNorm = (intermediate_output[0]+1) * 127.5\n",
    "show_intermediate_output(test_images[0], int_unNorm ) \n",
    "                            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model to calculate an intermediate layer using the test images\n",
    "layer_name = 'Conv4'\n",
    "intermediate_layer_model = Model(input=model.input,\n",
    "                                 output=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(test_images)\n",
    "int_unNorm = (intermediate_output[0]+1) * 127.5\n",
    "show_intermediate_output(test_images[0], int_unNorm ) \n",
    "                            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from keras.utils.visualize_util import plot\n",
    "from keras.models import load_model\n",
    "%matplotlib inline\n",
    "\n",
    "#visualize the model\n",
    "modelobj = load_model('model.h5')\n",
    "plot (modelobj, to_file='model.png')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(100, 100))\n",
    "image = cv2.imread('model.png')\n",
    "image = cv2.cvtColor (image, cv2.COLOR_BGR2RGB)\n",
    "plt.subplot(5, 5, 1)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
